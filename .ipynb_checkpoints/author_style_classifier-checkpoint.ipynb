{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e31586e781eb198f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:59:42.220873Z",
     "start_time": "2025-03-10T22:59:42.217391Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marioreyes/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd778856c636ac2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:59:42.260393Z",
     "start_time": "2025-03-10T22:59:42.238933Z"
    }
   },
   "outputs": [],
   "source": [
    "class AuthorStyleTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lowercase=True, remove_punctuation=False,\n",
    "                 remove_numbers=False, min_word_length=1,\n",
    "                 use_stylometric=True, use_ngram=False, use_matrix=False,\n",
    "                 ngram_sizes=(2,), max_features=1000, matrix_mode='binary'):\n",
    "        \"\"\"\n",
    "        Scikit-learn transformer for author style analysis with combined feature types.\n",
    "\n",
    "        Args:\n",
    "            lowercase: Whether to convert text to lowercase\n",
    "            remove_punctuation: Whether to remove punctuation\n",
    "            remove_numbers: Whether to remove numbers\n",
    "            min_word_length: Minimum length of words to keep\n",
    "\n",
    "            use_stylometric: Whether to include stylometric features\n",
    "            use_ngram: Whether to include n-gram features\n",
    "            use_matrix: Whether to include document-term matrix features\n",
    "\n",
    "            ngram_sizes: Tuple of n-gram sizes to extract\n",
    "            max_features: Maximum number of features per feature type\n",
    "            matrix_mode: Representation mode for matrix ('binary', 'count', 'freq', 'tfidf')\n",
    "        \"\"\"\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.min_word_length = min_word_length\n",
    "\n",
    "        self.use_stylometric = use_stylometric\n",
    "        self.use_ngram = use_ngram\n",
    "        self.use_matrix = use_matrix\n",
    "\n",
    "        self.ngram_sizes = ngram_sizes\n",
    "        self.max_features = max_features\n",
    "        self.matrix_mode = matrix_mode\n",
    "\n",
    "        self.punctuation_translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "        # Features data structures\n",
    "        self.vocabulary_ = None\n",
    "        self.feature_names_ = None\n",
    "        self.ngram_vocab_ = {}\n",
    "        self.doc_freq_ = None\n",
    "        self.num_docs_ = 0\n",
    "        self.stylo_feature_names_ = None\n",
    "        self.feature_indices_ = {}  # Track where each feature type begins in the combined array\n",
    "        self.nltk_stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a text sequence into words.\n",
    "\n",
    "        Args:\n",
    "            text: Input text sequence\n",
    "\n",
    "        Returns:\n",
    "            list: List of tokens\n",
    "        \"\"\"\n",
    "        # Apply preprocessing options\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "\n",
    "        if self.remove_punctuation:\n",
    "            text = text.translate(self.punctuation_translator)\n",
    "\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        # Split into tokens\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Filter by minimum length\n",
    "        tokens = [token for token in tokens if len(token) >= self.min_word_length and token not in self.nltk_stop_words]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def extract_ngrams(self, text, n=2):\n",
    "        \"\"\"\n",
    "        Extract n-grams from text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            n: Size of n-grams\n",
    "\n",
    "        Returns:\n",
    "            list: List of n-grams\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        units = tokens\n",
    "        ngrams = []\n",
    "        for i in range(len(units) - n + 1):\n",
    "            ngram = units[i:i + n]\n",
    "            ngrams.append(' '.join(ngram))\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def extract_stylometric_features(self, text):\n",
    "        \"\"\"\n",
    "        Extract stylometric features from text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of stylometric features\n",
    "        \"\"\"\n",
    "        original_text = text\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        # Calculate features\n",
    "        features = {}\n",
    "\n",
    "        # Average word length\n",
    "        if tokens:\n",
    "            features['avg_word_length'] = sum(len(token) for token in tokens) / len(tokens)\n",
    "        else:\n",
    "            features['avg_word_length'] = 0\n",
    "\n",
    "        # Sentence features\n",
    "        sentences = re.split(r'[.!?]+', original_text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "        if sentences:\n",
    "            # Average sentence length (in words)\n",
    "            features['avg_sentence_length'] = sum(len(s.split()) for s in sentences) / len(sentences)\n",
    "\n",
    "            # Sentence length variation\n",
    "            mean_len = features['avg_sentence_length']\n",
    "            variance = sum((len(s.split()) - mean_len) ** 2 for s in sentences) / len(sentences)\n",
    "            features['sentence_length_variance'] = variance\n",
    "        else:\n",
    "            features['avg_sentence_length'] = 0\n",
    "            features['sentence_length_variance'] = 0\n",
    "\n",
    "        # Lexical features\n",
    "        if tokens:\n",
    "            # Lexical diversity (unique words / total words)\n",
    "            features['lexical_diversity'] = len(set(tokens)) / len(tokens)\n",
    "\n",
    "            # Hapax legomena (words occurring only once)\n",
    "            word_counts = Counter(tokens)\n",
    "            features['hapax_percentage'] = sum(1 for w, c in word_counts.items() if c == 1) / len(tokens)\n",
    "        else:\n",
    "            features['lexical_diversity'] = 0\n",
    "            features['hapax_percentage'] = 0\n",
    "\n",
    "        # Punctuation density\n",
    "        punctuation_count = sum(1 for char in original_text if char in string.punctuation)\n",
    "        features['punctuation_density'] = punctuation_count / len(original_text) if original_text else 0\n",
    "\n",
    "        # N-gram features\n",
    "        if tokens:\n",
    "            # Word n-grams\n",
    "            for n in self.ngram_sizes:\n",
    "                if len(tokens) >= n:\n",
    "                    word_ngrams = self.extract_ngrams(text, n=n)\n",
    "                    word_ngram_counts = Counter(word_ngrams)\n",
    "\n",
    "                    # Top n-grams frequency\n",
    "                    top_ngrams = word_ngram_counts.most_common(5)\n",
    "                    for i, (ngram, count) in enumerate(top_ngrams):\n",
    "                        features[f'word_{n}gram_{i + 1}'] = count / len(word_ngrams) if word_ngrams else 0\n",
    "\n",
    "                    # N-gram diversity\n",
    "                    features[f'word_{n}gram_diversity'] = len(word_ngram_counts) / len(\n",
    "                        word_ngrams) if word_ngrams else 0\n",
    "\n",
    "        return features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the data.\n",
    "\n",
    "        Args:\n",
    "            X: List of text samples\n",
    "            y: Ignored (included for scikit-learn compatibility)\n",
    "\n",
    "        Returns:\n",
    "            self: The fitted transformer\n",
    "        \"\"\"\n",
    "        self.num_docs_ = len(X)\n",
    "        all_feature_names = []\n",
    "        current_index = 0\n",
    "\n",
    "        # Fit stylometric features if enabled\n",
    "        if self.use_stylometric:\n",
    "            sample_features = self.extract_stylometric_features(X[0])\n",
    "            self.stylo_feature_names_ = list(sample_features.keys())\n",
    "            all_feature_names.extend(self.stylo_feature_names_)\n",
    "            self.feature_indices_['stylometric'] = (current_index, current_index + len(self.stylo_feature_names_))\n",
    "            current_index += len(self.stylo_feature_names_)\n",
    "\n",
    "        # Fit n-gram features if enabled\n",
    "        if self.use_ngram:\n",
    "            for n in self.ngram_sizes:\n",
    "                all_ngrams = []\n",
    "\n",
    "                for text in X:\n",
    "                    text_ngrams = self.extract_ngrams(text, n=n)\n",
    "                    all_ngrams.extend(text_ngrams)\n",
    "\n",
    "                ngram_counts = Counter(all_ngrams)\n",
    "                top_ngrams = ngram_counts.most_common(self.max_features)\n",
    "                self.ngram_vocab_[n] = {ngram: idx for idx, (ngram, _) in enumerate(top_ngrams)}\n",
    "\n",
    "                ngram_feature_names = [f\"word_{n}gram_{ngram}\" for ngram in self.ngram_vocab_[n].keys()]\n",
    "                all_feature_names.extend(ngram_feature_names)\n",
    "\n",
    "                self.feature_indices_[f'ngram_{n}'] = (current_index, current_index + len(ngram_feature_names))\n",
    "                current_index += len(ngram_feature_names)\n",
    "\n",
    "        # Fit matrix features if enabled\n",
    "        if self.use_matrix:\n",
    "            all_tokens = []\n",
    "            for text in X:\n",
    "                all_tokens.extend(self.tokenize(text))\n",
    "\n",
    "            word_counts = Counter(all_tokens)\n",
    "            most_common = word_counts.most_common(self.max_features)\n",
    "            self.vocabulary_ = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
    "\n",
    "            matrix_feature_names = [f\"term_{word}\" for word in self.vocabulary_.keys()]\n",
    "            all_feature_names.extend(matrix_feature_names)\n",
    "\n",
    "            self.feature_indices_['matrix'] = (current_index, current_index + len(matrix_feature_names))\n",
    "            current_index += len(matrix_feature_names)\n",
    "\n",
    "            # For TF-IDF, calculate document frequencies\n",
    "            if self.matrix_mode == 'tfidf':\n",
    "                self.doc_freq_ = Counter()\n",
    "                for text in X:\n",
    "                    tokens = self.tokenize(text)\n",
    "                    unique_tokens = set(t for t in tokens if t in self.vocabulary_)\n",
    "                    for token in unique_tokens:\n",
    "                        self.doc_freq_[token] += 1\n",
    "\n",
    "        self.feature_names_ = all_feature_names\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data.\n",
    "\n",
    "        Args:\n",
    "            X: List of text samples\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Transformed features\n",
    "        \"\"\"\n",
    "        if not self.feature_names_:\n",
    "            raise ValueError(\"Transformer must be fitted before transform\")\n",
    "\n",
    "        n_samples = len(X)\n",
    "        n_features = len(self.feature_names_)\n",
    "        result = np.zeros((n_samples, n_features))\n",
    "\n",
    "        # Transform stylometric features if enabled\n",
    "        if self.use_stylometric:\n",
    "            start_idx, end_idx = self.feature_indices_['stylometric']\n",
    "            for i, text in enumerate(X):\n",
    "                features = self.extract_stylometric_features(text)\n",
    "                for j, feature_name in enumerate(self.stylo_feature_names_):\n",
    "                    result[i, start_idx + j] = features.get(feature_name, 0)\n",
    "\n",
    "        # Transform n-gram features if enabled\n",
    "        if self.use_ngram:\n",
    "            for n in self.ngram_sizes:\n",
    "                start_idx, end_idx = self.feature_indices_[f'ngram_{n}']\n",
    "\n",
    "                for i, text in enumerate(X):\n",
    "                    text_ngrams = self.extract_ngrams(text, n=n)\n",
    "                    text_ngram_counts = Counter(text_ngrams)\n",
    "\n",
    "                    for ngram, count in text_ngram_counts.items():\n",
    "                        if ngram in self.ngram_vocab_[n]:\n",
    "                            j = self.ngram_vocab_[n][ngram]\n",
    "                            result[i, start_idx + j] = count\n",
    "\n",
    "        # Transform matrix features if enabled\n",
    "        if self.use_matrix:\n",
    "            start_idx, end_idx = self.feature_indices_['matrix']\n",
    "\n",
    "            for i, text in enumerate(X):\n",
    "                tokens = self.tokenize(text)\n",
    "                token_counts = Counter(t for t in tokens if t in self.vocabulary_)\n",
    "\n",
    "                for token, count in token_counts.items():\n",
    "                    if token in self.vocabulary_:\n",
    "                        j = self.vocabulary_[token]\n",
    "\n",
    "                        if self.matrix_mode == 'binary':\n",
    "                            result[i, start_idx + j] = 1\n",
    "                        elif self.matrix_mode == 'count':\n",
    "                            result[i, start_idx + j] = count\n",
    "                        elif self.matrix_mode == 'freq':\n",
    "                            result[i, start_idx + j] = count / len(tokens) if tokens else 0\n",
    "                        elif self.matrix_mode == 'tfidf':\n",
    "                            # TF (term frequency) * IDF (inverse document frequency)\n",
    "                            tf = count / len(tokens) if tokens else 0\n",
    "                            idf = np.log(self.num_docs_ / (1 + self.doc_freq_[token]))\n",
    "                            result[i, start_idx + j] = tf * idf\n",
    "\n",
    "        return result\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the data and transform it.\n",
    "\n",
    "        Args:\n",
    "            X: List of text samples\n",
    "            y: Ignored (included for scikit-learn compatibility)\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Transformed features\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        \"\"\"\n",
    "        Get output feature names for transformation.\n",
    "\n",
    "        Returns:\n",
    "            list: Feature names\n",
    "        \"\"\"\n",
    "        if not self.feature_names_:\n",
    "            raise ValueError(\"Transformer must be fitted before getting feature names\")\n",
    "\n",
    "        return np.array(self.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eee265a488c40d23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:59:42.269502Z",
     "start_time": "2025-03-10T22:59:42.263236Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_author_style_pipeline(classifier='ridge', param_grid=None, cv=3, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Build a pipeline for author style classification.\n",
    "\n",
    "    Args:\n",
    "        classifier: Type of classifier ('ridge' or 'forest')\n",
    "        param_grid: Optional parameter grid (uses default if None)\n",
    "        cv: Number of cross-validation folds\n",
    "        n_jobs: Number of parallel jobs for grid search\n",
    "\n",
    "    Returns:\n",
    "        Fitted GridSearchCV object\n",
    "    \"\"\"\n",
    "    # Define the feature extractor\n",
    "    feature_extractor = AuthorStyleTransformer(\n",
    "        lowercase=True,\n",
    "        remove_punctuation=True,\n",
    "        min_word_length=3,\n",
    "        use_stylometric=True,\n",
    "        use_ngram=True,\n",
    "        use_matrix=True,\n",
    "        ngram_sizes=(2, 3, ),\n",
    "        max_features=20000,\n",
    "        matrix_mode='tfidf'\n",
    "    )\n",
    "\n",
    "    # Create classifier\n",
    "    if classifier == 'ridge':\n",
    "        clf = RidgeClassifier(random_state=42)\n",
    "    elif classifier == 'forest':\n",
    "        clf = RandomForestClassifier(random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported classifier: {classifier}\")\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('features', feature_extractor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "\n",
    "    # Define default parameter grids if none provided\n",
    "    if param_grid is None:\n",
    "        if classifier == 'ridge':\n",
    "            param_grid = {\n",
    "                'features__use_stylometric': [True],\n",
    "                'features__matrix_mode': ['tfidf'],\n",
    "                'features__matrix_mode': [(1, 2, ), (2, 3, )]\n",
    "                \n",
    "            }\n",
    "        elif classifier == 'forest':\n",
    "            param_grid = {\n",
    "                'features__use_stylometric': [True],\n",
    "                'features__matrix_mode': ['tfidf'],\n",
    "                'features__matrix_mode': [(1, 2, 3, )],\n",
    "                'classifier__class_weight': ['balanced'],\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__max_depth': [10, 20]\n",
    "            }\n",
    "\n",
    "    # Create grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "\n",
    "    return grid_search\n",
    "\n",
    "\n",
    "def train_and_evaluate(X, y, classifier='ridge', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Train and evaluate the author style classification pipeline.\n",
    "\n",
    "    Args:\n",
    "        X: List of text samples\n",
    "        y: List of corresponding labels\n",
    "        classifier: Type of classifier ('ridge' or 'forest')\n",
    "        test_size: Proportion of data to use for testing\n",
    "\n",
    "    Returns:\n",
    "        Fitted GridSearchCV object and performance metrics\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Build and fit pipeline\n",
    "    grid_search = build_author_style_pipeline(classifier=classifier)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Extract feature importance if using RandomForest\n",
    "    if classifier == 'forest':\n",
    "        feature_names = best_model.named_steps['features'].get_feature_names_out()\n",
    "        importances = best_model.named_steps['classifier'].feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        print(\"\\nTop 10 most important features:\")\n",
    "        for i in range(min(10, len(feature_names))):\n",
    "            print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "    return grid_search, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4016b2df61dcf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:59:42.515503Z",
     "start_time": "2025-03-10T22:59:42.277343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de datos = 181452\n",
      "Duplicados: 3\n",
      "Duplicados: 0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train.csv', sep = ',')\n",
    "print(f'Total de datos = {data.size}')\n",
    "\n",
    "print(f'Duplicados: {data.duplicated().sum()}')\n",
    "data = data.drop_duplicates()\n",
    "print(f'Duplicados: {data.duplicated().sum()}')\n",
    "\n",
    "X_ = data['text']\n",
    "y_ = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46946f6c-6c16-4eab-8a6e-1bd3eeaae58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 60018)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e59e80841bfe5a8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T22:59:46.620849Z",
     "start_time": "2025-03-10T22:59:42.523572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Best parameters: {'features__matrix_mode': (1, 2), 'features__use_stylometric': True}\n",
      "Best cross-validation score: 0.5801\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.87      0.68      5220\n",
      "           1       0.61      0.60      0.60      3253\n",
      "           2       0.76      0.26      0.38      1917\n",
      "           3       0.85      0.07      0.14       889\n",
      "           4       0.83      0.06      0.12       818\n",
      "\n",
      "    accuracy                           0.59     12097\n",
      "   macro avg       0.72      0.37      0.38     12097\n",
      "weighted avg       0.64      0.59      0.53     12097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ridge_model, ridge_report = train_and_evaluate(X_.values, y_.values, classifier='ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fd3f6c0-44f0-4ecc-bbe3-a8cc1322577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best parameters: {'classifier__class_weight': 'balanced', 'classifier__max_depth': 20, 'classifier__n_estimators': 200, 'features__matrix_mode': (1, 2, 3), 'features__use_stylometric': True}\n",
      "Best cross-validation score: 0.4129\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.34      0.47      5220\n",
      "           1       0.46      0.62      0.53      3253\n",
      "           2       0.48      0.25      0.33      1917\n",
      "           3       0.26      0.19      0.22       889\n",
      "           4       0.15      0.66      0.24       818\n",
      "\n",
      "    accuracy                           0.41     12097\n",
      "   macro avg       0.42      0.41      0.36     12097\n",
      "weighted avg       0.56      0.41      0.43     12097\n",
      "\n",
      "\n",
      "Top 10 most important features:\n",
      "punctuation_density: 0.1206\n",
      "avg_sentence_length: 0.0429\n",
      "sentence_length_variance: 0.0297\n",
      "word_3gram_1: 0.0274\n",
      "avg_word_length: 0.0273\n",
      "word_2gram_4: 0.0260\n",
      "word_2gram_said the: 0.0259\n",
      "word_2gram_2: 0.0258\n",
      "word_2gram_said mr.: 0.0256\n",
      "word_3gram_4: 0.0252\n"
     ]
    }
   ],
   "source": [
    "ridge_model, ridge_report = train_and_evaluate(X_.values, y_.values, classifier='forest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
