{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marioreyes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=300, window=10, min_count=2, epochs=20):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        tokenized_texts = []\n",
    "        for text in X:\n",
    "            words = word_tokenize(text.lower())\n",
    "            words = [word for word in words if len(word) > 1]\n",
    "            tokenized_texts.append(words)\n",
    "        \n",
    "\n",
    "        self.model = Word2Vec(sentences=tokenized_texts,\n",
    "                            vector_size=self.vector_size,\n",
    "                            window=self.window,\n",
    "                            min_count=self.min_count,\n",
    "                            workers=4,\n",
    "                            epochs=self.epochs,\n",
    "                            sg=1)  # Skip-gram\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not fitted yet!\")\n",
    "        \n",
    "        features = np.zeros((len(X), self.vector_size))\n",
    "        \n",
    "        for i, text in enumerate(X):\n",
    "            word_vectors = []\n",
    "            words = word_tokenize(text.lower())\n",
    "            words = [word for word in words if len(word) > 1]\n",
    "            \n",
    "            for word in words:\n",
    "                if word in self.model.wv:\n",
    "                    word_vectors.append(self.model.wv[word])\n",
    "            \n",
    "            if word_vectors:\n",
    "                features[i] = np.mean(word_vectors, axis=0)\n",
    "                features[i] = features[i] / np.linalg.norm(features[i])\n",
    "                \n",
    "        return features\n",
    "\n",
    "class VocabularyRichnessTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = np.zeros((len(X), 4))\n",
    "        \n",
    "        for i, text in enumerate(X):\n",
    "            words = word_tokenize(text.lower())\n",
    "            unique_words = set(words)\n",
    "            \n",
    "            if words:\n",
    "                # Type-token ratio\n",
    "                features[i, 0] = len(unique_words) / len(words)\n",
    "                \n",
    "                # Hapax legomena ratio (words appearing only once)\n",
    "                word_counts = Counter(words)\n",
    "                hapax = sum(1 for word, count in word_counts.items() if count == 1)\n",
    "                features[i, 1] = hapax / len(words)\n",
    "                \n",
    "                # Average word length\n",
    "                features[i, 2] = sum(len(word) for word in words) / len(words)\n",
    "                \n",
    "                # Vocabulary size\n",
    "                features[i, 3] = len(unique_words)\n",
    "        \n",
    "        return features\n",
    "\n",
    "class FunctionWordTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.function_words = [\n",
    "            'the', 'of', 'and', 'a', 'to', 'in', 'is', 'you', 'that', 'it',\n",
    "            'he', 'was', 'for', 'on', 'are', 'as', 'with', 'his', 'they',\n",
    "            'I', 'at', 'be', 'this', 'have', 'from', 'or', 'one', 'had',\n",
    "            'by', 'but', 'not', 'what', 'all', 'were', 'we', 'when', 'your',\n",
    "            'can', 'said', 'there', 'use', 'an', 'each', 'which', 'she', 'do',\n",
    "            'how', 'their', 'if', 'will', 'up', 'other', 'about', 'out', 'many'\n",
    "        ]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = np.zeros((len(X), len(self.function_words)))\n",
    "        \n",
    "        for i, text in enumerate(X):\n",
    "            words = word_tokenize(text.lower())\n",
    "            total_words = len(words) if words else 1\n",
    "            \n",
    "            for j, func_word in enumerate(self.function_words):\n",
    "                count = sum(1 for word in words if word == func_word)\n",
    "                features[i, j] = count / total_words\n",
    "                \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('word2vec', Word2VecTransformer(vector_size=300, window=15, min_count=2, epochs=20)),\n",
    "        ('vocab_richness', VocabularyRichnessTransformer()),\n",
    "        ('function_words', FunctionWordTransformer())\n",
    "    ], transformer_weights={\n",
    "        'word2vec': 0.6,\n",
    "        'vocab_richness': 0.2,\n",
    "        'function_words': 0.2\n",
    "    }))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de datos = 181452\n",
      "Duplicados: 3\n",
      "Duplicados: 0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train.csv', sep=',')\n",
    "print(f'Total de datos = {data.size}')\n",
    "\n",
    "print(f'Duplicados: {data.duplicated().sum()}')\n",
    "data = data.drop_duplicates()\n",
    "print(f'Duplicados: {data.duplicated().sum()}')\n",
    "\n",
    "X_ = data['text']\n",
    "y_ = data['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marioreyes/Documents/p_repositories/uniandes/FPLN_team3/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution before oversampling:\n",
      "Counter({0: 20899, 1: 12978, 2: 7746, 3: 3521, 4: 3240})\n",
      "\n",
      "Applying SMOTE oversampling...\n",
      "\n",
      "Class distribution after oversampling:\n",
      "Counter({0: 20899, 1: 20899, 2: 20899, 4: 20899, 3: 20899})\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features...\")\n",
    "X_train_features = feature_pipeline.fit_transform(X_train, y_train)\n",
    "X_test_features = feature_pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"\\nClass distribution before oversampling:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "print(\"\\nApplying SMOTE oversampling...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_features, y_train)\n",
    "\n",
    "\n",
    "print(\"\\nClass distribution after oversampling:\")\n",
    "print(Counter(y_train_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the classifier...\n",
      "Making predictions...\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.85      0.89      5197\n",
      "           1       0.88      0.87      0.88      3285\n",
      "           2       0.78      0.88      0.82      1839\n",
      "           3       0.72      0.83      0.77       926\n",
      "           4       0.81      0.94      0.87       850\n",
      "\n",
      "    accuracy                           0.86     12097\n",
      "   macro avg       0.82      0.87      0.85     12097\n",
      "weighted avg       0.87      0.86      0.87     12097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining the classifier...\")\n",
    "classifier = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=2000,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs',\n",
    "    n_jobs=-1\n",
    ")\n",
    "classifier.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred = classifier.predict(X_test_features)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
